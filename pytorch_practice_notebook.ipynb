{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoenoo/FedGPT/blob/main/pytorch_practice_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "PyTorch Practice Notebook - ML Interview Preparation\n",
        "\n",
        "35 Implementation Questions with Learning Objectives"
      ],
      "metadata": {
        "id": "2rOOrASnJyky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ],
      "metadata": {
        "id": "-0Zsw0BoJeqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 1: TENSOR FUNDAMENTALS (Questions 1-8)"
      ],
      "metadata": {
        "id": "z9ATSJ41JpqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Question 1: Basic Tensor Operations\n",
        "def tensor_basics():\n",
        "    \"\"\"\n",
        "    Create a 3x4 tensor with random values, then:\n",
        "    1. Add 5 to all elements\n",
        "    2. Multiply by 2\n",
        "    3. Take the square root\n",
        "    4. Calculate the mean across dimension 1\n",
        "\n",
        "    Learning: Tensor creation, broadcasting, reduction operations\n",
        "    \"\"\"\n",
        "    # TODO: Implement this function\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "QELqcLiDJhic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2: Tensor Indexing and Slicing\n",
        "def tensor_indexing():\n",
        "    \"\"\"\n",
        "    Given a 5x5 tensor:\n",
        "    1. Extract the diagonal elements\n",
        "    2. Set the last row to zeros\n",
        "    3. Extract every other column\n",
        "    4. Use advanced indexing to select specific elements\n",
        "\n",
        "    Learning: PyTorch indexing, slicing, masking\n",
        "    \"\"\"\n",
        "    x = torch.randn(5, 5)\n",
        "    # TODO: Implement the operations above\n",
        "    pass"
      ],
      "metadata": {
        "id": "viEfsrorJnJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3: Gradient Computation\n",
        "def gradient_computation():\n",
        "    \"\"\"\n",
        "    Create tensors x and y, compute z = x^2 + y^3, and find gradients.\n",
        "    Include a case where you need to retain_graph=True\n",
        "\n",
        "    Learning: Autograd, computational graphs, gradient retention\n",
        "    \"\"\"\n",
        "    # TODO: Implement gradient computation\n",
        "    pass"
      ],
      "metadata": {
        "id": "XghTrr7EJojh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 4: In-place vs Out-of-place Operations\n",
        "def inplace_operations():\n",
        "    \"\"\"\n",
        "    Demonstrate the difference between in-place and out-of-place operations.\n",
        "    Show what happens to gradients with in-place operations.\n",
        "\n",
        "    Learning: Memory efficiency, gradient computation issues with in-place ops\n",
        "    \"\"\"\n",
        "    # TODO: Show examples of both types and their effects on gradients\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "wSnxQN98J7Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 5: Broadcasting Rules\n",
        "def broadcasting_examples():\n",
        "    \"\"\"\n",
        "    Create examples that demonstrate PyTorch broadcasting rules:\n",
        "    1. (3,1) + (1,4) = (3,4)\n",
        "    2. (2,3,1) * (1,1,4) = (2,3,4)\n",
        "    3. Show a case that would fail broadcasting\n",
        "\n",
        "    Learning: Broadcasting mechanics, common pitfalls\n",
        "    \"\"\"\n",
        "    # TODO: Implement broadcasting examples\n",
        "    pass"
      ],
      "metadata": {
        "id": "XvAt5oXMJ_if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Tensor Memory Layout\n",
        "def memory_layout():\n",
        "    \"\"\"\n",
        "    Create tensors with different memory layouts (contiguous vs non-contiguous).\n",
        "    Use .view() vs .reshape() appropriately.\n",
        "\n",
        "    Learning: Memory efficiency, when to use contiguous()\n",
        "    \"\"\"\n",
        "    # TODO: Demonstrate memory layout concepts\n",
        "    pass"
      ],
      "metadata": {
        "id": "g9Rzd2v5KBBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Device Management\n",
        "def device_operations():\n",
        "    \"\"\"\n",
        "    Write code that works on both CPU and GPU.\n",
        "    Move tensors between devices and handle device compatibility.\n",
        "\n",
        "    Learning: CUDA operations, device-agnostic code\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # TODO: Implement device management examples\n",
        "    pass"
      ],
      "metadata": {
        "id": "sm56DVP3KCkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Custom Tensor Operations (INTENTIONAL BUG)\n",
        "def custom_tensor_ops():\n",
        "    \"\"\"\n",
        "    BUG ALERT: This function has an intentional error in tensor dimension handling.\n",
        "    Find and fix the bug!\n",
        "    \"\"\"\n",
        "    def matrix_multiply_bug(a, b):\n",
        "        # This has a bug - find it!\n",
        "        result = torch.zeros(a.shape[0], b.shape[0])  # BUG HERE\n",
        "        for i in range(a.shape[0]):\n",
        "            for j in range(b.shape[1]):\n",
        "                result[i, j] = torch.sum(a[i, :] * b[:, j])\n",
        "        return result\n",
        "\n",
        "    # TODO: Find the bug and fix it, then test your fix\n",
        "    pass"
      ],
      "metadata": {
        "id": "VW6EpQeNKFo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Other practice questions\n",
        "\n",
        "\"\"\"\n",
        "Create a 2D tensor of shape (3, 4) with random values. Print its shape and dtype.\n",
        "\n",
        "Perform element-wise addition, multiplication on tensors of shape (2, 3).\n",
        "\n",
        "Perform matrix multiplication between (2, 3) and (3, 4) tensors.\n",
        "\n",
        "Reshape a tensor from (4, 3) → (2, 6) → back. Add assertion.\n",
        "\n",
        "Convert NumPy array to tensor and back.\n",
        "\n",
        "Normalize a tensor to zero mean and unit variance. Fix divide-by-zero.\n",
        "\n",
        "Create a one-hot encoded tensor from indices [0, 2, 1] for 3 classes.\n",
        "\n",
        "Stack and concatenate 3 tensors of shape (2, 2).\n",
        "\n",
        "Use broadcasting to subtract row-wise means from a 2D tensor.\n",
        "\n",
        "Implement a custom function to clip tensor values between 0 and 1.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tFf54YiiLgni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 2: NEURAL NETWORK FUNDAMENTALS (Questions 9-16)"
      ],
      "metadata": {
        "id": "bZchNab1KIRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Linear Layer Implementation\n",
        "class LinearLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement a linear layer from scratch using nn.Parameter.\n",
        "    Include proper initialization and forward pass.\n",
        "\n",
        "    Learning: nn.Module structure, Parameter vs regular tensors\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super().__init__()\n",
        "        # TODO: Initialize weights and bias as Parameters\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement forward pass\n",
        "        pass"
      ],
      "metadata": {
        "id": "xgEXSBixKMLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: Activation Functions\n",
        "class ActivationFunctions:\n",
        "    \"\"\"\n",
        "    Implement various activation functions from scratch and compare with PyTorch versions.\n",
        "\n",
        "    Learning: Activation function mathematics, numerical stability\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def custom_relu(x):\n",
        "        # TODO: Implement ReLU from scratch\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def custom_sigmoid(x):\n",
        "        # TODO: Implement sigmoid with numerical stability considerations\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def custom_tanh(x):\n",
        "        # TODO: Implement tanh from scratch\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def leaky_relu(x, negative_slope=0.01):\n",
        "        # TODO: Implement Leaky ReLU\n",
        "        pass"
      ],
      "metadata": {
        "id": "wdJ_lhwMKO8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 11: Loss Functions\n",
        "class CustomLosses:\n",
        "    \"\"\"\n",
        "    Implement common loss functions from scratch.\n",
        "\n",
        "    Learning: Loss function mathematics, reduction strategies\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def mse_loss(predictions, targets, reduction='mean'):\n",
        "        # TODO: Implement MSE loss\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_entropy_loss(logits, targets):\n",
        "        # TODO: Implement cross-entropy loss with numerical stability\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def binary_cross_entropy(predictions, targets):\n",
        "        # TODO: Implement BCE loss\n",
        "        pass"
      ],
      "metadata": {
        "id": "wMXUwh3DKRah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 12: Multi-Layer Perceptron\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Create a configurable MLP with:\n",
        "    - Variable number of hidden layers\n",
        "    - Dropout\n",
        "    - Batch normalization option\n",
        "    - Different activation functions\n",
        "\n",
        "    Learning: Model architecture design, regularization techniques\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.0, use_batchnorm=False):\n",
        "        super().__init__()\n",
        "        # TODO: Build the network architecture\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement forward pass with optional batchnorm and dropout\n",
        "        pass"
      ],
      "metadata": {
        "id": "RKs7eGqtKTIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 13: Custom Dataset Class\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Create a dataset class that can handle both regression and classification data.\n",
        "    Include data augmentation options.\n",
        "\n",
        "    Learning: Dataset creation, data preprocessing pipeline\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, transform=None, task_type='classification'):\n",
        "        # TODO: Initialize dataset\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO: Return dataset length\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO: Return single sample with optional transforms\n",
        "        pass"
      ],
      "metadata": {
        "id": "tnO3_pQ1KUoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 14: Training Loop Implementation\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Implement a flexible training loop with:\n",
        "    - Training and validation phases\n",
        "    - Metrics tracking\n",
        "    - Early stopping\n",
        "    - Learning rate scheduling\n",
        "\n",
        "    Learning: Training best practices, monitoring, optimization\n",
        "    \"\"\"\n",
        "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def train_epoch(self):\n",
        "        # TODO: Implement one training epoch\n",
        "        pass\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        # TODO: Implement validation epoch\n",
        "        pass\n",
        "\n",
        "    def train(self, num_epochs, early_stopping_patience=None):\n",
        "        # TODO: Implement full training loop with early stopping\n",
        "        pass"
      ],
      "metadata": {
        "id": "T5lxB4tpKXNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 15: Regularization Techniques (INTENTIONAL BUG)\n",
        "class RegularizedModel(nn.Module):\n",
        "    \"\"\"\n",
        "    BUG ALERT: This model has issues with dropout usage during training/evaluation.\n",
        "    Find and fix the bugs!\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # BUG: Always applies dropout, even during evaluation\n",
        "        self.layers.train()  # BUG HERE\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "fOQd9j02KZtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 16: Weight Initialization\n",
        "def initialize_weights(model):\n",
        "    \"\"\"\n",
        "    Implement different weight initialization strategies:\n",
        "    - Xavier/Glorot initialization\n",
        "    - He initialization\n",
        "    - Custom initialization based on layer type\n",
        "\n",
        "    Learning: Initialization importance, different strategies\n",
        "    \"\"\"\n",
        "    # TODO: Implement various initialization schemes\n",
        "    pass"
      ],
      "metadata": {
        "id": "Q1YnSnCnKbSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Other practice questions\n",
        "\n",
        "\"\"\"\n",
        "# Create a tensor with requires_grad, compute y = x^2 + 3x + 1, call backward().\n",
        "# Use torch.autograd.grad to compute gradients without .backward().\n",
        "# Implement manual gradient descent to learn y = 2x.\n",
        "# Add training loop without zeroing gradients. Fix it.\n",
        "# Write a custom autograd function for absolute value.\n",
        "# Print and interpret gradients using hooks.\n",
        "# Implement a custom Dataset class that loads data from CSV.\n",
        "# Use DataLoader to batch, shuffle and load the dataset with batch size 8.\n",
        "# Use torchvision transforms (ToTensor, Normalize) on image dataset.\n",
        "# Apply composed transforms using transforms.Compose().\n",
        "# Write a collate_fn that pads variable-length sequences.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "muzPVKIXL2g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 3: ADVANCED ARCHITECTURES (Questions 17-24)"
      ],
      "metadata": {
        "id": "ZfmFSuQvKcp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 17: Convolutional Neural Network\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Build a CNN for image classification with:\n",
        "    - Multiple conv layers with pooling\n",
        "    - Batch normalization\n",
        "    - Global average pooling\n",
        "\n",
        "    Learning: CNN architecture, feature extraction\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, input_channels=3):\n",
        "        super().__init__()\n",
        "        # TODO: Define CNN architecture\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement forward pass\n",
        "        pass"
      ],
      "metadata": {
        "id": "D_EvPwxyKf_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 18: Residual Connections\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement a residual block with optional downsampling.\n",
        "\n",
        "    Learning: Skip connections, gradient flow, identity mapping\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        # TODO: Implement residual block\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement forward with residual connection\n",
        "        pass"
      ],
      "metadata": {
        "id": "6lB_p6cSKhOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 19: Attention Mechanism\n",
        "class SimpleAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement a basic attention mechanism.\n",
        "\n",
        "    Learning: Attention computation, weighted aggregation\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        # TODO: Define attention parameters\n",
        "        pass\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # TODO: Implement attention computation\n",
        "        pass"
      ],
      "metadata": {
        "id": "ndOwn1O0KiZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 20: LSTM from Scratch\n",
        "class CustomLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement LSTM cell from scratch using the mathematical formulation.\n",
        "\n",
        "    Learning: RNN mechanics, gate mechanisms, hidden state evolution\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        # TODO: Define LSTM parameters (gates, weights, biases)\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # TODO: Implement LSTM forward pass\n",
        "        pass"
      ],
      "metadata": {
        "id": "YRwLM5QKKjh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 21: Sequence-to-Sequence Model\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Build a simple encoder-decoder model.\n",
        "\n",
        "    Learning: Sequence modeling, encoder-decoder architecture\n",
        "    \"\"\"\n",
        "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size):\n",
        "        super().__init__()\n",
        "        # TODO: Define encoder and decoder\n",
        "        pass\n",
        "\n",
        "    def forward(self, src, tgt=None):\n",
        "        # TODO: Implement seq2seq forward pass\n",
        "        pass"
      ],
      "metadata": {
        "id": "X-DMAUVgKkvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 22: Variational Autoencoder\n",
        "class VAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement a Variational Autoencoder with:\n",
        "    - Encoder (recognition network)\n",
        "    - Decoder (generative network)\n",
        "    - Reparameterization trick\n",
        "\n",
        "    Learning: Generative models, variational inference, latent representations\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, latent_size):\n",
        "        super().__init__()\n",
        "        # TODO: Define encoder and decoder networks\n",
        "        pass\n",
        "\n",
        "    def encode(self, x):\n",
        "        # TODO: Encode input to latent parameters\n",
        "        pass\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        # TODO: Implement reparameterization trick\n",
        "        pass\n",
        "\n",
        "    def decode(self, z):\n",
        "        # TODO: Decode latent to reconstruction\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Full VAE forward pass\n",
        "        pass"
      ],
      "metadata": {
        "id": "4_HATdVnKmaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 23: Transformer Block (INTENTIONAL BUG)\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    BUG ALERT: This transformer block has issues with attention mask application.\n",
        "    Find and fix the bug!\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # BUG: Incorrect mask application\n",
        "        attn_out, _ = self.self_attn(x, x, x, attn_mask=mask)  # BUG HERE - mask shape/application\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out))\n",
        "        return x"
      ],
      "metadata": {
        "id": "mtieNfpbKn9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 24: Model Ensemble\n",
        "class ModelEnsemble:\n",
        "    \"\"\"\n",
        "    Create an ensemble of models with different voting strategies.\n",
        "\n",
        "    Learning: Model combination, prediction aggregation, ensemble methods\n",
        "    \"\"\"\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "\n",
        "    def predict_average(self, x):\n",
        "        # TODO: Average predictions from all models\n",
        "        pass\n",
        "\n",
        "    def predict_weighted(self, x, weights):\n",
        "        # TODO: Weighted average based on model performance\n",
        "        pass\n",
        "\n",
        "    def predict_majority_vote(self, x):\n",
        "        # TODO: Majority voting for classification\n",
        "        pass"
      ],
      "metadata": {
        "id": "aHTIX78LKp0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 4: OPTIMIZATION AND TRAINING (Questions 25-30)"
      ],
      "metadata": {
        "id": "HkvshaN1KrHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 25: Custom Optimizer\n",
        "class SGDMomentum(optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Implement SGD with momentum from scratch.\n",
        "\n",
        "    Learning: Optimizer mechanics, parameter updates, momentum\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=0.01, momentum=0.9):\n",
        "        defaults = dict(lr=lr, momentum=momentum)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        # TODO: Implement SGD with momentum step\n",
        "        pass"
      ],
      "metadata": {
        "id": "fsUYPH1BKtoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 26: Learning Rate Scheduling\n",
        "def create_lr_scheduler(optimizer, schedule_type='cosine', **kwargs):\n",
        "    \"\"\"\n",
        "    Create different types of learning rate schedulers.\n",
        "\n",
        "    Learning: Learning rate scheduling strategies, training optimization\n",
        "    \"\"\"\n",
        "    # TODO: Implement different scheduler types\n",
        "    pass"
      ],
      "metadata": {
        "id": "mV9Kz6AGKvQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 27: Gradient Clipping and Monitoring\n",
        "class GradientMonitor:\n",
        "    \"\"\"\n",
        "    Monitor and clip gradients during training.\n",
        "\n",
        "    Learning: Gradient explosion, monitoring training health\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.grad_norms = []\n",
        "\n",
        "    def clip_gradients(self, max_norm):\n",
        "        # TODO: Implement gradient clipping\n",
        "        pass\n",
        "\n",
        "    def log_gradient_norms(self):\n",
        "        # TODO: Log gradient norms for monitoring\n",
        "        pass"
      ],
      "metadata": {
        "id": "bvRyQkFKKwbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 28: Mixed Precision Training\n",
        "def setup_mixed_precision_training(model, optimizer):\n",
        "    \"\"\"\n",
        "    Set up automatic mixed precision training.\n",
        "\n",
        "    Learning: Memory optimization, numerical stability, AMP\n",
        "    \"\"\"\n",
        "    # TODO: Implement AMP setup and usage\n",
        "    pass"
      ],
      "metadata": {
        "id": "ng8d6SPdKxi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 29: Model Checkpointing and Loading\n",
        "class ModelCheckpoint:\n",
        "    \"\"\"\n",
        "    Implement model checkpointing with state management.\n",
        "\n",
        "    Learning: Model persistence, training resumption, state management\n",
        "    \"\"\"\n",
        "    def __init__(self, model, optimizer, scheduler=None):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def save_checkpoint(self, filepath, epoch, loss, metrics=None):\n",
        "        # TODO: Save complete training state\n",
        "        pass\n",
        "\n",
        "    def load_checkpoint(self, filepath):\n",
        "        # TODO: Load and restore training state\n",
        "        pass"
      ],
      "metadata": {
        "id": "n6Xr2IsbKyxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 30: Distributed Training Setup\n",
        "def setup_distributed_training():\n",
        "    \"\"\"\n",
        "    Set up basic distributed training configuration.\n",
        "\n",
        "    Learning: Multi-GPU training, distributed optimization\n",
        "    \"\"\"\n",
        "    # TODO: Implement distributed training setup\n",
        "    pass"
      ],
      "metadata": {
        "id": "oONfp3j0Kz8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Other practice questions\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Write a complete training loop with forward → loss → backward → optimizer step.\n",
        "\n",
        "Add validation loop that computes accuracy and loss.\n",
        "\n",
        "Save and load model weights with torch.save and torch.load.\n",
        "\n",
        "Add early stopping logic if validation loss does not improve for 3 epochs.\n",
        "\n",
        "Add torch.no_grad() context during evaluation. Explain why it's needed.\n",
        "\n",
        "Add learning rate scheduler (StepLR or ReduceLROnPlateau).\n",
        "\n",
        "Add tqdm progress bar and print loss, accuracy live.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YDf8bx7TMLjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION 5: DEBUGGING AND OPTIMIZATION (Questions 31-35)"
      ],
      "metadata": {
        "id": "4GgScKZ0K1PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 31: Memory Profiling\n",
        "def profile_model_memory(model, input_shape, batch_size=32):\n",
        "    \"\"\"\n",
        "    Profile memory usage of a model during forward and backward passes.\n",
        "\n",
        "    Learning: Memory optimization, profiling tools\n",
        "    \"\"\"\n",
        "    # TODO: Implement memory profiling\n",
        "    pass"
      ],
      "metadata": {
        "id": "0yybnJerK3bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 32: Model Debugging Tools\n",
        "class ModelDebugger:\n",
        "    \"\"\"\n",
        "    Create debugging tools for model inspection.\n",
        "\n",
        "    Learning: Debugging techniques, model introspection\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.hooks = []\n",
        "\n",
        "    def register_hooks(self):\n",
        "        # TODO: Register forward/backward hooks for debugging\n",
        "        pass\n",
        "\n",
        "    def check_gradients(self):\n",
        "        # TODO: Check for gradient issues (vanishing/exploding)\n",
        "        pass\n",
        "\n",
        "    def visualize_activations(self, x):\n",
        "        # TODO: Visualize layer activations\n",
        "        pass"
      ],
      "metadata": {
        "id": "qjMzM7tHK4y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 33: Performance Optimization\n",
        "def optimize_model_inference(model):\n",
        "    \"\"\"\n",
        "    Apply various optimization techniques for inference.\n",
        "\n",
        "    Learning: JIT compilation, quantization, optimization techniques\n",
        "    \"\"\"\n",
        "    # TODO: Implement TorchScript, quantization, etc.\n",
        "    pass"
      ],
      "metadata": {
        "id": "Fpp1ikFoK5-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 34: Custom Loss with Regularization (INTENTIONAL BUG)\n",
        "class CustomLossWithRegularization(nn.Module):\n",
        "    \"\"\"\n",
        "    BUG ALERT: This loss function has numerical stability issues.\n",
        "    Find and fix the bugs!\n",
        "    \"\"\"\n",
        "    def __init__(self, l1_weight=0.01, l2_weight=0.01):\n",
        "        super().__init__()\n",
        "        self.l1_weight = l1_weight\n",
        "        self.l2_weight = l2_weight\n",
        "\n",
        "    def forward(self, predictions, targets, model):\n",
        "        # Primary loss\n",
        "        mse_loss = F.mse_loss(predictions, targets)\n",
        "\n",
        "        # L1 regularization - BUG: inefficient computation\n",
        "        l1_reg = sum(p.abs().sum() for p in model.parameters())  # BUG: should use more efficient method\n",
        "\n",
        "        # L2 regularization - BUG: incorrect computation\n",
        "        l2_reg = sum(p.pow(2).sum() for p in model.parameters() if p.grad is not None)  # BUG HERE\n",
        "\n",
        "        # BUG: Potential division by zero\n",
        "        total_loss = mse_loss + self.l1_weight * l1_reg + self.l2_weight * l2_reg / len(list(model.parameters()))\n",
        "\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "LwSPHxkbK7Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Question 35: Advanced Training Techniques\n",
        "class AdvancedTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Extend the basic trainer with advanced techniques:\n",
        "    - Curriculum learning\n",
        "    - Progressive resizing\n",
        "    - Test-time augmentation\n",
        "    - Model averaging\n",
        "\n",
        "    Learning: Advanced training strategies, performance optimization\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # TODO: Add advanced training components\n",
        "\n",
        "    def curriculum_learning_step(self, epoch):\n",
        "        # TODO: Implement curriculum learning\n",
        "        pass\n",
        "\n",
        "    def test_time_augmentation(self, x, num_augmentations=5):\n",
        "        # TODO: Implement TTA for better predictions\n",
        "        pass\n",
        "\n",
        "    def exponential_moving_average_update(self, decay=0.999):\n",
        "        # TODO: Update EMA of model weights\n",
        "        pass"
      ],
      "metadata": {
        "id": "cW-GWvHiK88y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LEARNING EXERCISES AND FOLLOW-UP QUESTIONS"
      ],
      "metadata": {
        "id": "ljE3H26eLE3y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGn_jgtkIq_I"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "FOLLOW-UP QUESTIONS TO DEEPEN YOUR UNDERSTANDING:\n",
        "\n",
        "Section 1 - Tensor Fundamentals:\n",
        "1. What's the difference between .view() and .reshape()? When might each fail?\n",
        "2. How does PyTorch's autograd system handle in-place operations, and why?\n",
        "3. What are the memory implications of keeping computational graphs?\n",
        "4. How does broadcasting work internally, and what are its limitations?\n",
        "\n",
        "Section 2 - Neural Network Fundamentals:\n",
        "5. Why is proper weight initialization crucial? Compare Xavier vs He initialization.\n",
        "6. What happens to gradients in very deep networks, and how do residual connections help?\n",
        "7. How do different optimizers (SGD, Adam, RMSprop) differ in their update rules?\n",
        "8. What are the trade-offs between different activation functions?\n",
        "\n",
        "Section 3 - Advanced Architectures:\n",
        "9. How does the attention mechanism solve the bottleneck problem in seq2seq models?\n",
        "10. What makes LSTMs better than vanilla RNNs at handling long sequences?\n",
        "11. How does the reparameterization trick in VAEs enable backpropagation?\n",
        "12. What are the key differences between different normalization techniques?\n",
        "\n",
        "Section 4 - Optimization and Training:\n",
        "13. How do different learning rate schedules affect convergence?\n",
        "14. What are the benefits and challenges of mixed precision training?\n",
        "15. How does gradient clipping prevent exploding gradients?\n",
        "16. What are the considerations for distributed training?\n",
        "\n",
        "Section 5 - Debugging and Optimization:\n",
        "17. How can you identify and fix common training issues (overfitting, underfitting)?\n",
        "18. What are the trade-offs between model accuracy and inference speed?\n",
        "19. How do different quantization techniques affect model performance?\n",
        "20. What debugging techniques help identify gradient flow issues?\n",
        "\n",
        "IMPLEMENTATION CHALLENGES:\n",
        "- Try to implement each skeleton class completely\n",
        "- Find and fix all intentional bugs\n",
        "- Optimize the implementations for better performance\n",
        "- Add error handling and input validation\n",
        "- Create comprehensive test cases\n",
        "- Implement additional features beyond the basic requirements\n",
        "\n",
        "INTERVIEW PREPARATION TIPS:\n",
        "- Understand the mathematical foundations behind each implementation\n",
        "- Be able to explain trade-offs and design decisions\n",
        "- Practice coding these from scratch without looking at documentation\n",
        "- Understand when to use which technique and why\n",
        "- Be prepared to debug and optimize existing code\n",
        "\"\"\"\n"
      ]
    }
  ]
}